

<!DOCTYPE html>
<!-- saved from url=(0057)https://eo4geocourses.github.io/PLUS_OBIA-Introduction/#/ -->
<html prefix="eo4geo: http://bok.eo4geo.eu/" lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


	<title>EO-Lab Deep learning </title>
	<meta property="dc:title" content="EOlab Deep learning">
	<meta property="dc:creator" content="Vanessa Streifeneder & Florian Albrecht, University of Salzburg, SpaSe">
	<meta property="dc:publisher" content="University of Salzburg">
	<meta property="dc:subject" content="Deep learning for EO data processing on the EO Cloud with TensorFlow and Pytorch">
	
	<!-- Metadata need to be updated -->

	<meta property="dc:abstract"
		content="">

	<meta property="dc:tableOfContents" content="(1) Introduction; (2) Image interpretation and perception; (3) Basic concepts of hierarchy theory; (4) Knowledge representation; (5) Image segmentation; (6) Object-based classification (incl. class modelling); (7) Accuracy assessment (incl. object validity; (8) Outlook: OBIA for non-image data">
	<meta property="dc:description"
		content="">
	<meta property="dc:contributor" content="Vanessa Streifeneder,Florian Albrecht,Stefan Lang">
	<meta property="dc:created" content="">
	<meta property="dc:type" content="teaching material">
	<meta property="dc:format" content="html">
	<meta property="dc:language" content="EN">
	<meta property="dc:SizeOrDuration" content="">
	<meta property="dc:audience" content="">
	<meta property="dc:educationLevel" content="">
	<meta property="dc:source" content="">
	<meta property="dc:rightsHolder" content="">
	<meta property="dc:license" content="https://creativecommons.org/licenses/by-sa/4.0/deed.en">


	<link rel="stylesheet" href="./dist/reveal.css">
	
	<link rel="stylesheet" href="./css/img_animation.css">
	<link rel="stylesheet" href="./css/redBox.css"> <!--RedBox Chapter: 1-->
	<link rel="stylesheet" href="./css/greyBox.css"> <!--GreyBox Chapter: 1-->
	<link rel="stylesheet" href="./css/bubble.css"> <!--Speech-Bubble-->
	<link rel="stylesheet" href="./css/dot.css"> <!--Dot-->
	<!-- CHOOSE BETWEEN 2 EO4GEO TEMPLATE THEMES:-->
	<link rel="stylesheet" href="./css/eo4geo.css" id="theme"> <!-- white background design -->
	<link rel="stylesheet" href="./css/theme/simplemenu.css">
	<link rel="stylesheet" href="plugin/node_modules/reveal.js-menu/font-awesome/css/all.css">
	
	<link href="https://fonts.googleapis.com/css?family=Roboto Slab" rel="stylesheet">
	

	<!--<base target="_blank">-->
	<base href="." target="_blank"> <!-- All links in the presentation are opened in a seperate tab"-->
	<script type="text/javascript" src="./plugin/notes/notes.js"></script>
	<script type="text/javascript" src="./plugin/highlight/highlight.js"></script>

	
	<script type="text/javascript" src="./dist/reveal.js"></script>
	<script  type="text/javascript" src="./plugin/node_modules/reveal.js-menu/menu.js"></script>
	
	<script type="text/javascript" src="./plugin/notes-server/client.js"></script>

	<style type="text/css">
		.hljs-ln {
			border-collapse: collapse
		}

		.hljs-ln td {
			padding: 0
		}

		.hljs-ln-n:before {
			content: attr(data-line-number)
		}
	</style>
</head>
<script>

	var link = document.createElement( 'link' );
	link.rel = 'stylesheet';
	link.type = 'text/css';
	link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
	document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>

<body class="reveal-viewpoint">
	

	<div class="reveal slide center has-vertical-slides has-horizontal-slides ready" role="application"
		data-transition-speed="default" data-background-transition="fade" style="cursor: pointer;">

		<!-- FORMATTING FOR HEADER -->

		<div class="EOlabLogo" style="width: 11%;height : auto"></div> 
			<div class="slides" style="width:960px; height:700px; inset: 50% auto auto 50%; transform: translate(-50%,-50%) scale (0.478);" >

			<!-- AUTHORS: DEFINE CONTENT ON YOUR FIRST SLIDE -->
			
<!--DL Course with EO-Lab-->

		<section section  data-background-image="background.jgp" data-background-opacity=1 data-background-size="100%" data-background-repeat="no-repeat">
			
		
				<h1 >Deep Learning for EO data processing on the EO Cloud with TensorFlow and Pytorch</h1>
				<div style="color:#ebf4fe">
				<small>Florian Albrecht <sub>2</sub>, Vanessa Streifeneder <sub>1</sub>, Stefan Lang<sub>1</sub></small>
				<p></p>
				<p>
					<small> <sub> 1</sub> University of Salzburg</small>			
				
					<small> <sub> 2</sub> Spatial Service GmbH</small>			
				</p>
				
				</div>
				
				

				
		</section>
		<!--Learning objectives-->
		<section id= "Chapter 1.0" data-background-color="white">
			<h2>1. Learning objectives</h2>
			<div style="text-align: left;font-size: 90%;">
				<ul>
					<li>This course will support you to gain the following skills</li>
					<ul>
						<li> Understand the deep learning workflow and concepts for image classification with EO data</li>
						<li> Understand how the deep learning methods can be implemented with TensorFlow and PyTorch</li>
						<li> Apply a TensorFlow or PyTorch script to EO-Lab resources</li>
					</ul>
				</ul>
			</div>
			
		</section>
		
		<!-- Slide Navigation-->
		<section id="Chapter 2.0" data-background-color="white">
			<h2>2.  How to navigate this slide deck</h2>
			<div style="text-align: left; font-size: 65%; width: 50%">
				<ul>
					<li>Arrangement of slides in this course material</li>
					<ul>
						<li> Main course sections:<p style="margin-top: 0%;"> Horizontal from left to right</p></li>
						<li> Content slides of sections: <p style="margin-top: 0%;"> Vertically from top to bottom</p></li>
					</ul>
					<li>Arrows for navigation (lower right)</li>
					<li> Button to show content (lower left) </li>
				</ul>
			</div>
			<div >
				<figure style="position: absolute; right: -5%; top: 20%; width:50%">
					<img src="./Images/SlideArrangement.PNG" style="background: none; border: none; box-shadow: none; ">
				</figure>
				<figure style="position: absolute; right: 10%; bottom: -10%; width:50%">
					<img src="./Images/Slide_Content_Symbol.PNG" style="background: none; border: none; box-shadow: none; ">
				</figure>
				<figure style="position: absolute; right: 0%; bottom: -10%; width:15%">
					<img src="./Images/Slide_Content_Arrows.PNG" style="background: none; border: none; box-shadow: none; ">
				</figure>
			</div>
			
		</section>
		<!--Content-->
		<section id="Chapter 3.0"data-background-color="white">
			<h2>3. Content</h2>
			
			<div  >
				<ol >
					
				<div >
					<li> <a href="#/4/0">Preface</a>
					</li>
				<div >
				<li > <a href="#/5/0"> Introduction to Deep Learning (DL) for Earth observation (EO)</a> 
				</li>	
			</div>	
			<div >								
				<li >
				<a href="#/6/0">The Convolutional Neural Network (CNN)</a>	
				</li>
			</div>
			<div >	
				<li >
				<a href="#/7/0"> Application example with TensorFlow</a>	
				</li>
			</div>
			<div >	
				<li >
				<a href="#/8/0"> Application example with PyTorch</a>	
				</li>
			</div>
			<div >	
				<li > <a href="#/9/0"> Conclusion</a></li>
			</div>					
			</ol>		
						
		</div>
			
		
		</section>
		<!-- Preface-->
		<section id="Chapter 4.0" data-background-color="white">
			<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
				<p style="position: absolute; bottom:-100%; font-size:40%; color:white"> Sentinel-2,  Salzburg, Ã–sterreich, at 18. Mai 2022 </p>
				
					<h2 style="background-color: white; margin: 5%; "> 4. Preface</h2>
				
				</section>
		

		<!--Set-Up-->

		<section id="Chapter 4.1">
			<h2>4.1 The EO-Lab context</h2>
			<ul style="font-size:60% ; width:80%_; text-align: left;">
				<li>  The course material has been optimized for the following setup:</li>
				<ul>
					<li>
						The course participants have their own <b>cloud resources on EO-Lab</b>.
					</li>
					<li>
						The course participants have set up a <b> virtual machine (VM) with GPU support</b>  on the platform for access via remote control.
					</li>
					<li>
						The VM has a <b>programming environment installed </b> that enables scripting and running of code with <b>Jupyter notebooks</b>.
					</li>
				</ul>
				
					<li>For setting up your EO-Lab resources accordingly, information is available here</li>
					<ul>
					<li>
						FAQs in the EO-Lab help, accessible at <a href="https://eo-lab.org/de/help/" > EO-Lab Help</a>
					</li>
					<li>
						The EO-Lab documentation, accessible at <a href=" https://knowledgebase.eo-lab.org"> EO-Lab knowledgebase</a>
					</li>
				</ul>
			</ul>
			
		</section>

		<!-- Work Environment-->
		<section id="Chapter 4.2">
			<h2> 4.2 How to arrange your work environment</h2>
			<ul style="text-align: left; font-size:55%; margin-top: 0%; margin-bottom: 0%;">
				<li >This course focuses on scripting for Deep Learning.</li>
				<li>Open the course material and your VM with the programming environment in two separate windows next to each other.</li>
				<li> The link to the TensorFlow script is available on the course site in the section Course material.</li>
			</ul>
			<figure>
				<img src="./Images/Slide3_figure_DL_course_setup.png" style="background: none; border: none; box-shadow: none; width:80%; margin-top:-5%; margin-bottom: 0%;">
			</figure>
			
		</section>

		<!-- Chapter End-->
		<section id="Chapter 4.3" data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
			
		</section>
	</section>
<!-- Chapter 1: Introduction-->
		<section id="Chapter 5.0"  >
			<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
					<p style="position: absolute; bottom:-200%; font-size:40%; color:white"> Sentinel-2,  Salzburg, Ã–sterreich, at 18. Mai 2022 </p>
					
						<h2 style="background-color: white; margin: 5%; "> 5. Introduction to AI for EO</h2>
					
					
			</section>
			


		<!-- DL for classification of EO data--> 

			<section id="Chapter 5.1 " data-background-color="white">
				<h2 > 5.1 Deep Learning (DL) for classification of Earth observation (EO) data</h2>
				<div style="font-size: 50%; margin-right: 40%;">
					<ul>
					
						<li>
							Objective
						</li>
						<ul>
							<li>
								Classifying land cover in satellite imagery using Convolutional Neural Networks (CNNs)
							</li>
						</ul>						
						<li>
							The DL approach applies
						</li>
						<ul>
							
							<li>
								Supervised learning- requires training data as input
							</li>
						
							<li>
								Machine learning- after training of algorithm follows prediction by algorithm
							</li>
						
							<li>
								Type of classification task for land cover mapping- Semantic segmentation
							</li>
						
						</ul>

					</ul>
				</div>
				<p style="position: absolute; right: 0%; top:45%; width:50%; font-size: 50%;"> Supervised learning</p>
				<figure style="position: absolute; right: -10%; top:43%; width:45%">
					<img src="./Images/1_1_Supervised_learning.png" style=" background: none; border: none; box-shadow: none">
				</figure>
				<p style="position: absolute; right: 0%; top:75%; width:50%; font-size: 50%;"> Supervised machine learning</p>
				<figure style="position: absolute; right: 0%; top:75%; width:40%">
					<img src="./Images/1_2_supervised_machine_learning.png" style=" background: none; border: none; box-shadow: none">
				</figure>
				<div style="position: absolute; right: -20%; bottom:-18%; width:60%; font-size: small;"><a href="http://www.eo4geo.eu/training/the-rise-of-artificial-intelligence-for-earth-observation/">Source: The rise of artificial intelligence for earth observation</a> </div>
				

			</section>

	
		<!--Classification tasks for DL-->
			<section id="Chapter 5.2" data-background-color="white">
				<h2 >5.2 Classification tasks for Deep Learning</h2>
				<div style="display:inline;">
			
				
						<div class="redBox"> Image classification/ recognition</div>				
						<div class="redBox" > Object detection</div>				
				
					
						<div class="redBox"   >Semantic segmentation </div>	
					
						<div class="redBox" > Instance segmentation</div>				
				
					<div style="display: inline; float:left">
						
						<div class="bubble">images are recognized according to scene content</div>
						<div class="bubble" >objects located in image matrix</div>				
						<div class="bubble" >image is segmented by assigning class probabilities to each pixel</div>
						<div class="bubble" >individual objects delineated</div>
						
					</div>
		
				</div>
				<figure>
					<img src="./Images/2_1_image_recognition.png" style=" background: none; border: none; box-shadow: none">
				</figure>
				<div>
					<p style="text-align: left; font-size: 40%; position: absolute; bottom:5%"> Examples of image recognition tasks (Hoeser and Kuenzer, 2010)</p>
				</div>					
				

			</section>

		<!--DL workflow-->
		<section id="Chapter 5.3" data-background-color="white">
			<h2 >5.3 The Deep Learning workflow </h2>
			<div style="display: inline">	
				<span class="dot" style="float:left"></span>
				<div style="float:left; ">&#x2192;</div>
				<div class="greyBox"> Collect input data</div>	
				<div style="float:left; ">&#x2192;</div>
						
				<div class="greyBox" >Select classificator and parameters </div>
				<div style="float:left; ">&#x2192;</div>
			
			
				<div class="greyBox" > Perform classification</div>
				<div style="float:left; ">&#x2192;</div>
				
			
				<div class="greyBox"> Assess classification</div>
				<div style="float:left; ">&#x2192;</div>
			
				<div class="greyBox">Further use</div>
				<div style="float:left; ">&#x2192;</div>
				<span class="dot" style="float:left; background-color:red"></span>
			
			</div>
			
		</section>

		<!--DL workflow: Input Data-->
		<section id="Chapter 5.4" data-background-color="white">
			<h2 > 5.4 The Deep Learning workflow </h2>
			<figure>
				<img src="./Images/DeepLearningWorkflow.png" style=" background: none; border: none; box-shadow: none;">
			</figure>
	
			

		</section>

		<!--Section End: Quiz-->
		<section id= "Chapter 5.5" data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
					
		
						<h2 style="background-color: white; margin: 2%; ">  Quiz 1: <p style="font-size: 70%;">Understanding deep learning tasks for image classification </p> </h2>
						
						
						
							
						<iframe data-src="./Tests/Define tasks of classification for images.html" width="1498" height="311" frameborder="0" background-color="white"  2allowfullscreen="allowfullscreen" title="Quiz: Understanding deep learning tasks for image classification"></iframe>
						
						
							
			
					</section>

		</section>

<!-- Chapter 2: CNN-->
		<section id="Chapter 6.0" >
			<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
				
					<h2 style="color:rgb(0, 119, 255); background-color: white; margin: 5%; "> 6. The Convolutional Neural Network architecture (CNN)</h2>
			
					
			</section>

			<!-- Architecture-->
			<section id="Chapter 6.1" data-background-color="white">
				<h2> 6.1 The CNN Architecture</h2>
				<div style="width: 50%; font-size: 50%; text-align: left;">
					<ul>
						<li>Constructing feature maps </li>
						<ul>
							
							<li>
								Convolution (with filters & kernels) and pooling are applied to the input image to create the feature maps of a multi-layer CNN
							</li>
						
						</ul>
						
						<li>Fully connected convolutional network (FCN) </li>
						<ul>
							<li>All nodes of the lower layer are connected to all nodes of the upper layer</li>
							<li>'Deep' networks (that stack multiple layers and FCNs) are suitable to learn complex functions such as the modelling of semantic segmentation.</li>
						</ul>
					
					</ul>
				
				</div>
				<figure style="position: absolute; right: 0%; top:12%; width:40%; ">
					<p style="font-size: 40%; text-align:left"><i>Stepwise reduction of the size of feature maps in a CNN </i></p>
					<img src="./Images/The-LeNet-5-Architecture-a-convolutional-neural-network.png" style="background: none; border: none; box-shadow: none; width:220%">
					<p style="font-size: 40%; text-align:left"><i>Multi-layer deep fully connected network</i></p>
					<img src="./Images/FCN_Ramsundar_Zadeh.png" style="background: none; border: none; box-shadow: none; width:50%">
				</figure>
								
			</section>

			<!-- U-Net-->
			<section id="Chapter 6.2" data-background-color="white">
				<h2> 6.2 U-Net model for semantic segmentation</h2>
				<div  style="width: 50%; font-size: 50%; text-align: left;">
					<ul>
						<li> The U-Net model in the figure to the right combines an encoder with a decoder. </li>
					
						<li>
							<b>Encoder - Contracting analysis path (left)</b>
						</li>
						<ul>
							<li>
								Encodes feature representation at multiple scales (i.e. identies the semantic classes that are present in the input image)
							</li>

						</ul>
					
						<li>
							<b>Decoder - Expansive synthesis path (right) </b>
						</li>
						<ul>
							<li>
								Projects the feature representation learned by the encoder to the original pixel space
							</li>
							<li>
								Ends with assigning a semantic class to each pixel
							</li>
						</ul>
					
					</ul>
				
				</div>
				<figure style="position: absolute; right: -5%; top:25%; width:50%; ">
					<p style="font-size: 40%; text-align:center"><i>U-Net model architecture</i></p>
					<img src="./Images/u-net-architecture.png" style="background: none; border: none; box-shadow: none; width:220%">
				
				</figure>
				
			</section>

					
			<!--Section End: Quiz-->
			<section id= "Chapter 6.3" data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
				
				
				<h2 style="background-color: white; margin: 2%; ">  Quiz 2: <p style="font-size: 70%;">Understanding the U-Net model structure </p> </h2>
						
						
						
							
				<iframe data-src="./Tests/Understanding the U-Net model structure.html" width="1498" height="311" frameborder="0" background-color="white"  2allowfullscreen="allowfullscreen" title="Quiz: Understanding deep learning tasks for image classification"></iframe>
				
				
				
			</section>

		</section>
			
		
<!-- Chapter 3: Application- TensorFlow-->

		<section id="Chapter 7.0" >
				<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
					
						<h2 style="color:rgb(0, 119, 255); background-color: white; margin: 5%; "> 7. Application example with TensorFlow	</h2>
					
						
				</section>

			<!-- Application context-->
			<section id="Chapter 7.1" data-background-color="white">
				<h2> 7.1 Application context</h2>
				<div style="width: 40%; font-size: 50%; text-align: left;">
					<ul>
						<li>Opportunity on EO-Lab</li>
						<ul style="font-size: 80%">
							<li>
								Sentinel-2 archive available
							</li>
							<li>
								ESA World Cover available
							</li>
						</ul>
						<li>
							Test case for this course
						</li>
						<ul style="font-size: 80%">
							<li>
								Use ESA World Cover to train a DL model
							</li>
							<li>
								Apply trained model to Sentinel-2 image for each year from 2015 until 2022
							</li>
							<li>
								Analyse time series of derived land cover maps
							</li>

						</ul>
					</ul>
				</div>

				<figure style="position: absolute; right: 0%; top:15%; width:50%  ">
					<p style="font-size: 40%; text-align:center"><i>Comparing Sentinel-2 and ESA WorldCover</i></p>
					<img src="./Images/EO_Browser_EO_Lab.png" style="background: none; border: none; box-shadow: none; ">
				</figure>
				
			</section>

			<!--Prepare VM-->
			<!--<section id="Chapter 7.2" data-background-color="white">
				<h2>Prepare data on the VM</h2>
				<ul style="font-size: 60%; text-align:left">
					<li>
						Use <b>EO-Browser</b> to identify EO datasets 
					</li>
					<ul>
						<li>
							Go to an area of interest (AOI), e.g. east of Berlin
						</li>
						<li>
							Identify cloudfree Sentinel-2 images representative of each year and copy dataset paths
						</li>
					</ul>
					<li>
						Go to your <b>VM on EO-Lab</b> to search files in repositories
					</li>
					<ul>
						<li>
							The paths to the repositories can be looked up in the <a href="https://eo-lab.org/en/portfolio/"> EO-Lab Portfolio</a>
						</li>
						<li>
							Locate the S-2 image files in the path <i>/eodata/Sentinel-2/MSI/L1C</i>
						</li>
						<li>
							Locate the ESA WorldCover dataset in the path <i>/codede/auxdata/esa-worldcover-2020</i> 
						</li>
					</ul>
					<li>
						Store <b>local copies</b> for image subsets
					</li>
					<ul>
						<li>
							Use AOI to extract subsets from S-2 images and from ESA World Cover
						</li>
						<li>
							Store the datasets on your VM for further processing
						</li>
					</ul>
				</ul>
				
			</section>
		-->
		
			<!--Prepare Data for model training-->
			<section id="Chapter 7.3" data-background-color="white">
				<h2>7.2.1 Prepare Data on the VM </h2>
				<div style="text-align: left; font-size: 70%;">
					<li>Use the <strong>EO-Browser</strong> to identify the needed EO data</li>
					<li>
						Define study area (AOI) &#8594; In this case east of Berlin
					</li>
					<figure style="margin-top: 0%; background: none; border: none; box-shadow: none; width:70%">
						<img src="./Images/AOI.png">
					</figure>

				</div>
				
			</section>

			<!--Get Data for model training-->
			<section id="Chapter 7.4" data-background-color="white">
				<h2> 7.2.2 Prepare Data on the VM </h2>
				<div style="text-align: left; font-size: 50%; width:60%">
					<li>Use the <strong>EO-Browser</strong> to identify the needed EO data</li>
					<ol>
					<li>Define study area (AOI) &#8594; In this case east of Berlin</li>	
					
					<li>
						Download ESA WorldCover
					</li>
					<li>
						Identify and download cloud-free S2-Image from 2020 (should match the ESA Worldcover 2020)
					</li>
					</ol>
					<figure style="float: left;">
						<img src="./Images/EO_Browser_EO_Lab.png" style=" background: none; border: none; box-shadow: none; width:90%;  ">
						<img src="./Images/EO_Browser_ESA.png" style="margin-top: 0%; background: none; border: none; box-shadow: none; width:50%; position: absolute; right: -5%; bottom: 5% ">
					</figure>
				</div>
					
						<figure>
							<img src="./Images/EO_Browser_Link.png" style=" background: none; border: none; box-shadow: none; width:25%; position: absolute; top: 12%; right:5%  ">
						</figure>
					
					
			</section>

			<section id="Chapter 7.5" data-background-color="white">
				<h2>7.2.3 Prepare Data on the VM </h2>
				<div style="text-align: left; font-size: 70%;">
					<ul>
					<li >Using the EO-Lab VM to find data in the connected archive</li>
					
					<ul>
						<li>Search for the satellite image based on the date and  the copied link</li>
					</ul>
			
					</ul>
				</div>
					<figure style="float: left; position: absolute; left:-15%">
						<img src="./Images/EO_Archive_S2.png"  style=" background: none; border: none; box-shadow: none; width: 70%;" >
					
						<img src="./Images/EO-Lab_Archive_ESA.png" style="margin-top: 0%; background: none; border: none; box-shadow: none; width:60%; position: absolute; right: -35%; bottom: 15% ">
					
					</figure>
					
			</section>

			<section id="Chapter 7.6 " data-background-color="white">
				<h2>7.2.4 Prepare Data on the VM </h2>
				<div style="text-align: left; font-size: 60%;">
				<ul>
					<li> Example: Preparation and saving of S2 and ESA WorldCover images </li>
					<ul>
						<li> Using ArcGIS Pro or any other GIS, Image processing or programming tool</li>
					</ul>
					<li> Defining training and evaluation area</li>
					<li> Preparing for each area one image and one label-mask (ESA Worldcover) as raster, with the same extensions</li>
				</ul>
				</div>
				<div>
					<figure style="float: left; position: absolute; left:-5%; margin-top: 0%; ">
						<img src="./Images/AOI_images.png" style=" background: none; border: none; box-shadow: none; width: 70%; bottom: 8% ">
						<img src="./Images/AOI_label.png" style="margin-top: 0%; background: none; border: none; box-shadow: none; width:79%; position: absolute; right: -64%; bottom: 4% ">
					</figure>
				</div>
				
			</section>

			<!-- Introduction to TensorFlow-->
			<section id="Chapter 7.7" data-background-color="white">
				<h2>7.3 Short Introduction: TensorFlow</h2>
				<div style="text-align: left; font-size: 60%; width: 70%">
					<ul>
						<li>Framework for datastream oriented programming with a focus on machine learning</li>
						<li> Python-based and open-sourced</li>
						<li> Enables usage of GPUs</li>
						<li> TensorFlow: based on tensors (matrices)</li>
						<ul>
							<li>Picture height: 2</li>
							<li> Picture width: 5</li>
							<li> channels/ layers: 3 (can be bands containg spectral information but also semantic information such as labels)</li>
						</ul>
						<li> More information: <a href="http://devseed.com/tensorflow-eo-training/docs/Lesson1a_Intro_ML_NN_DL.html"> Tensorflow- EO- Training</a></li>
					</ul>
				</div>
				<div>
					<figure>
						<img src="./Images/TensorFlow_logo.png" style="margin-top: 0%; background: none; border: none; box-shadow: none; width:20%; position: absolute; top: 15%; right: 8%">
						<img src="./Images/TensorFlow.png" style="margin-top: 0%; background: none; border: none; box-shadow: none; width:40%; position: absolute; bottom: 5%; right: 0%">
					</figure>
				</div>
				
			</section>
			<!--Necessary Libaries-->
			<section id="Chapter 7.8" data-background-color="white">
				<h2>7.4.1 TensorFlow setups (example Jupyter Notebook) </h2>
				<div style="text-align: left; font-size: 60%; ">
					<ul>
						<li>TensorFlow v2.11.x: GPU is contained inside the package for older version install: tensorflow-gpu </li>
						<li> Check <strong>if</strong> TensorFlow uses GPU:</li>
						<ul style="color:rgb(0, 119, 255)">
							<li > <i>Import tensorflow as tf</i></li>
							<li> <i>tf.confg.list_physical_devices('GPU')</i></li>
						</ul>
						<li> Check <strong>which</strong> GPUs are used</li>
						<ul style="color:rgb(0, 119, 255)">
							<li > <i>tf.debugging.set_log_device_placement(True)</i></li>
							</ul>
						<li><strong>Determine</strong> which GPUs shall be used</li>
						<ul style="color:rgb(0, 119, 255)">
							<li ><i>tf.device(`/GPU:x`)</i> 	</li>
							</ul>
												
					</ul>
					<p style="margin-top: 5%; font-size: 70%;"> Closer Documentation: <a href="https://www.tensorflow.org/guide/gpu"> Use a GPU - TensorFlow</a></p>
					<p style="margin-top: 2%; font-size: 60%; "> Note: The example code can be used with any python editor or just as python script </p>
				</div>
				
			</section>

			<section id="Chapter 7.9" data-background-color="white">
				<h2>7.4.2 TensorFlow setups (example Jupyter Notebook) </h2>
				<figure style="position: absolute; top: 60%">
					<img src="./Images/ImportingLibaries.png"  style=" background: none; border: none; box-shadow: none; width:75%">
				</figure>
				
			</section>

			<!--Preparing Sample Data-1-->
			<section id="Chapter 7.10" data-background-color="white">
				<h2> 7.5.1 Preparation of Sample Data  </h2>
				<div style="text-align: left; font-size: 60%;">
					<ul>
						<li> Two possible ways</li>
						<ol>
							<li> Sample dataset of <strong> many different pictures and labels</strong> used for model training (e.g different single pictures of forests and additional label images) </li>
							<ul>
								<li> Creating a training dataset with TensorFlow using: <span style="color: rgb(0, 119, 255)"> <i> tf.data.Datasets</i></span> </li>
							</ul>
						
						<li>Using <strong>one Image</strong> (example)</li>
						<ul>
							<li> Splitting the S2 satellite image and the according ESA Worldcover 2020 Image (label) into smaller image patches</li>
							<ul>
								<li>
									Python libary: patchify
								</li>
							</ul>
						</ul>
					</ol>
						</ul>
				</div>
				
			</section>

			<section id="Chapter 7.11" data-background-color="white">
				<h2> 7.5.2 Preparation of Sample Data </h2>
					
				
					<figure>
						<img src="./Images/ImportingTraining_3.png" style=" background: none; border: none; box-shadow: none; width:50%; margin-top: -5%;  ">
					</figure>
					
				

				</section>

			<section id="Chapter 7.12" data-background-color="white">
				<h2> 7.5.3 Preparation of Sample Data </h2>
	
				<div class="fragment fade-out" id="f6_4_f_5">
					<figure>
						<img src="./Images/ImportingLabels_2.png" style=" background: none; border: none; box-shadow: none; width:40%; margin-bottom: 0%;">
					</figure>
					<li style="text-align: left; font-size: 50%; margin-top: 0%;"> <strong>One-hot-encode labels</strong> to work with categorical parameters: Each class gets it own 'band', containing 0 (does not belong to class) and 1 (belongs to class) </li>
				</div>
				<div class="fragment fade-in">
					<figure>
						<img src="./Images/ImagePatches.png" style=" background: none; border: none; box-shadow: none; width:50%; position: absolute; top:25%; left:25%">
					</figure>
				</div>

				


			</section>

			<!--Preparing Sample Data-2-->
			<section id="Chapter 7.13" data-background-color="white">
				<h2> 7.5.4 Preparing Sample Data </h2>
				
					<figure>
						<img src="./Images/Testsplit.PNG" style=" background: none; border: none; box-shadow: none; width:50% "></figure>
					<div>
						<li style="text-align: left; font-size: 60%;">The <strong>validation dataset</strong> is used during the model training to tune the hyperparameters and to check if the model is over-/ underfitting</li>
					</div>	
				

			</section>

			<!--Preparing DL model and model run-->
			<section id="Chapter 7.14 " data-background-color="white">
				<h2>7.5 U-Net: Model settings  </h2>
				<div style=" text-align:left; font-size: 65%">
					<ul>
						<li>Import "Segmentation Models" from GitHub <a href="https://github.com/qubvel/segmentation_models">Qubvel - Segmentation Models</a></li>
						<li> Model parameters</li>
						<ul>
							<li> Backbone</li>
							<li> Weights</li>
							<li> Classes</li>
							<li> Patchsize (Image size of the training data)</li>
							<li> Image channels/ bands/ layers</li>
							<li> Activationfunction (softmax), optimizer (adam)</li>
							<li> Metrics (Information output regarding model performance and training)</li>
							<li> Epochs (Number of training iterations)</li>
							<li> Batchsize (Number of trainings samples which propagate through the model)</li>
						</ul>
					</ul>

				</div>
				
			</section>

			<section id="Chapter 7.15" data-background-color="white">
			<h2> 7.6 U-Net: Model Training </h2>
				<div>
					<li style="text-align: left; font-size: 60%;"> Specify U-Net model parameters and compile loss function, metrics and optimizer</li>
					<figure>
						<img src="./Images/prepareModel_new_2.png" style=" background: none; border: none; box-shadow: none; width: 50%; margin-top: 0%; margin-bottom:0%;  ">
					</figure>
					<li style="text-align: left; font-size: 60%;"> Start the model training using the model.fit () operation</li>
					<figure>
						<img src="./Images/model_fit.png" style=" background: none; border: none; box-shadow: none; width: 50%; margin-top: -3%; ">
					</figure>
				</div>
				
			</section>

			<!--Model Evaluation-->
			<section id="Chapter 7.16" data-background-color="white">
				<h2>7.7.1 U Net Model: Model Evaluation </h2>
				<ol>
					<li style="font-size: 60%"> Import the image and label raster of the evaluation dataset in the same way as the sample data</li>
				</ol>
				<figure>
					<img src="./Images/Evaluation_samples.png" style=" background: none; border: none; box-shadow: none;">
				</figure>
			
			</section>

			<!--Model Evaluation-->
			<section id="Chapter 7.17" data-background-color="white">
				<h2> 7.7.2 U-Net Model: Model Evaluation </h2>
				<ol>
				<li style="font-size: 60%"> Import the image and label raster of the evaluation dataset in the same way as the sample data</li>
				<li style="font-size: 60%"> Load the model</li>
				<li style="font-size: 60%"> Then evaluate the model as followed:</li>
			</ol>
				<figure>
					<img src="./Images/ModelEvaluation.PNG" style=" background: none; border: none; box-shadow: none;">
				</figure>
				
				
			</section>
				<!--U-Net Model Land cover classification-->
			<section id="Chapter 7.18" data-background-color="white">
				<h2> 7.8.1 U-Net Model: Land cover classification</h2>
							
					<figure style="width: 65%; margin-top: -3%; margin-left: 20%;">
						<img src="./Images/FurtherUse_1.PNG" style=" background: none; border: none; box-shadow: none; ">
						
					</figure>
				
			</section>

			<section id="Chapter 7.19"data-background-color="white">
				<h2> 7.8.2 U-Net Model: Land cover classification </h2>
				<div style="font-size: 50%; text-align: left; width:50%;margin-left: 0%;" >
					<ol >
						<li> Land cover classification according to their probability</li>
						<li> Safe the results for further use for instance as .tif</li>
						<li> Further analysis possible in GIS, Python etc.</li>
						<li> Further steps to increase the model performance:</li>
						<ul>
							<li> More trainings data, different patchsize</li>
							<li> Adapt model parameters (optimizer, loss-function etc.)</li>
							<li> Adapat or develope model architecture</li>
							<li> Try a different model</li>
						</ol>
						<p style="font-size: 60%;margin-top: 5%;">
							Closer Documentation: <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="http://www.eo4geo.eu/training/the-rise-of-artificial-intelligence-for-earth-observation/"> EO4GEO: The rise of AI for EO</a>, <a href="https://eo-college.org">EO-College</a>
						</p>
					</ul>

				</div>
				<figure>
					<img src="./Images/landcover_classification.PNG" style=" background: none; border: none; box-shadow: none; position: absolute; right: 0%; top:25%; width:50%">
				</figure>
				
			</section>

		
		<!--Code gesamt Jupyter-NB -->
			<section id="Chapter 7.20" data-background-color="white">
				<h2>7.9 Code: TensorFlow</h2>
				<iframe width="800" height="500" data-src="./plugin/LandcoverClassification.html" data-preload></iframe> 
				
			</section>

		<!--Section End: Quiz-->	
			<section id= "Chapter 7.21" data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
				
						
				<h2 style="background-color: white; margin: 2%; ">  Quiz 3: <p style="font-size: 70%;">Understanding the impact of available processing capacities on the DL workflow</p> </h2>
						
						
						
							
				<iframe data-src="./Tests/Understanding the impact of available processing capacities on the DL workflow.html" width="1498" height="311" frameborder="0" background-color="white"  2allowfullscreen="allowfullscreen" title="Quiz: Understanding deep learning tasks for image classification"></iframe>
				
				
					
			
			</section>
		</section>

<!-- Chapter 4 : Application example with PyTorch-->
	
<section id="Chapter 8.0" >
	<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
		
			<h2 style="color:rgb(0, 119, 255); background-color: white; margin: 5%; ">8. Application example with PyTorch </h2>
	
			
	</section>
	<!--Pytorch-->
	<section id="Chapter 8.1 "data-background-color="white">
		<h2> 8.1 PyTorch</h2>
		<ul style="font-size: 70%;  text-align: left; width: 60%; margin-left: -10%;">
			<li>
				Purpose:

				<ul>
					<li>
						Prepare a Deep Learning model in PyTorch for land cover classification
					</li>
				</ul>
			</li>
			<li>
				PyTorch:
				<ul>
					<li>
						Tensor libary for DL using GPUs and CPUs 
					</li>
					<li>
						Enables computer vision applications with EO data 
					</li>
					<li>
						More information on <a href="https://pytorch.org/"> PyTorch.org</a>
					</li>
				</ul>
			</li>
		</ul>
			
		<figure style="position: absolute; right: -10%; top:25%">
			
			<img src="./Images/PyTorch_Logo.png" style="width:80%; background: none; border: none; box-shadow: none;" >
		</figure>


	</section>

	<!-- PyTorch Installation-->
	<section id="Chapter 8.2 "data-background-color="white">
		<h2>8.2 PyTorch Installation</h2>
		<ul style="font-size: 60%;  text-align: left; width: 50%; margin-left: -30%;">
			<li>
				Check avilable CUDA library on your machine	
				<ul style="font-size: 90%;">
					<li>
						Windows: Got to control panel
					</li>
					<li>
						Linux: Type the command "nivdia-smi" in the terminal
					</li>
				</ul>
				<li>
					Select corresponding PyTorch version on <a href=" https://pytorch.org/ "> PyTorch.org</a>

				</li>
				<li>
					Run the command generated by the selection dialogue

				</li>
			</li>
	
		</ul>
		<figure style="position: absolute; right:-50%; top: 10%; width: 130%" >
			<img src="./Images/NVIDIA_CUDA.png" style="width:30%; background: none; border: none; box-shadow: none;" >
			
		</figure>


	</section>
<!--Training data - synchronize with model-->

<section id="Chapter 8.3 "data-background-color="white">
	<h2>8.3 Training data - Synchronize with model</h2>
	<ul style="font-size: 70%;  text-align: left; width: 45%; margin-left: -30%; margin-top: 5%;margin-bottom: 0%;">
		<li>
			<a href=" https://pytorch.org/docs/stable/data.html#map-style-datasets "> Map - style datasets</a> 
		</li>
		<li>
			<a href=" https://pytorch.org/docs/stable/data.html#iterable-style-datasets "> Iterable - style datasets</a>

		</li>
			
			</ul>
	<figure  >
		<img src="./Images/Utilis_Data_PyTorch.PNG" style="width:50%; background: none; border: none; box-shadow: none;" >
		
	</figure>


</section>

<!--Assessing Computational Resources-->

<section id="Chapter 8.4 "data-background-color="white">
	<h2>8.4 Assessing Computational Resources</h2>
	
	<figure >
		<img src="./Images/Assessing_Comp_Resources.PNG" style=" background: none; border: none; box-shadow: none;" >
		
	</figure>
</section>

<!-- Model Defintion --part 1-->
<section id="Chapter 8.5 "data-background-color="white">
	<h2> 8.5 Options for Model Definition</h2>
	<ul style="font-size: 70%;  text-align: left;  ">
		<li>
			User-defined models(regression, classification, and detection)
		</li>
		<li>
			Built-in models with pre-trained weights from <a href=" https://pytorch.org/vision/stable/index.html "> torchvision </a>
		</li>

	</ul>
	<figure >
		
		<img src="./Images/Torchvision.PNG" style="width:70%; background: none; border: none; box-shadow: none; margin-top: 2%;" >
	</figure>


</section>

<!-- Model Defintion --part 2   FEHLT!! --> 


<!--Training-->
<section id="Chapter 8.6 "data-background-color="white">
	<h2>8.6 PyTorch Instructions for Model Training</h2>
	
	<figure >
		<img src="./Images/Training_PyTorch.PNG" style=" background: none; border: none; box-shadow: none;" >
		
	</figure>
</section>



<!-- PyTorch:Code --> 

<section id="Chapter 8.7" data-background-color="white">
	<h2>8.7 Code: PyTorch</h2>
	<iframe width="800" height="500" data-src="./plugin/EO-Lab_Pytorch_script_example.html" data-preload></iframe> 
	
</section>
</section>

<!-- Chapter 5: Conclusion-->
	
		<section id="Chapter 9.0" >
				<section data-background-image="./Images/Satellitenaufnahme_total.jpg" data-background-opacity=1 data-background-size="100" data-background-repeat="no-repeat" >
					
						<h2 style="color:rgb(0, 119, 255); background-color: white; margin: 5%; ">9. Conclusion </h2>
				
						
				</section>
				
			<!-- Key messages -->
			<section id="Chapter 9.1 "data-background-color="white">
				<h2>9.1 Key messages of this course</h2>
				<ul style="font-size:60%">
					<li>
						This course presented:
					</li>
					<ul>					
							<li>
								DL workflow for image classification with EO data
							</li>
						
							<li>
								The DL concepts for land cover classification with CNNs
							</li>
						
							<li>
								An example application case that walks through the steps of DL workflow with a TensorFlow script and a PyTorch script
							</li>
						
							<li>
								Specifics of how to run the TensorFlow and PyTorch scripts on EO-Lab resources
							</li>						
					</ul>
				</ul>
				
			</section>	

			<!--Next steps -->
			<section id ="Chapter 9.2"data-background-color="white">
				<h2> 9.2 Next steps to pursue</h2>
				<div style="font-size: 75%; text-align: left;">
				<ul>
					<li> Opportunities for your next steps with the TensorFlow or PyTorch script</li>
					<ul style="font-size:75%;">
						<li> Apply the script to EO data of your favorite study area</li>
						<li> Adapt the script to other input data</li>
						<li> Reuse code blocks from the script in your own scripting projects</li>
					</ul>
					<li>
						Revisit further material on artificial intelligence for Earth Observation, such as:
						<ul style="font-size: 75%;">
							<li> <a href="https://developmentseed.org/tensorflow-eo-training/docs/index.html"> Deep Learning with TensorFlow: Tutorials for modeling LULC | by Lillianne Thomas and Ryan Avery | Development Seed </a> </li>
							<li> <a href="https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac"> Simple introduction to convolutional neural networks | by Matthew Stewart | Towards Data Science </a></li>
							<li> <a href="http://www.eo4geo.eu/training/the-rise-of-artificial-intelligence-for-earth-observation/"> The rise of artificial intelligence for earth observation | Planetek Italia | EO4GEO Webinar </a> </li>
							<li> <a href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47"> Understanding Semantic Segmentation with UNET | by Harshall Lamba | Towards Data Science </a> </li>
						</ul>
					</li>
				</ul>
			</div>
			
			</section>

			<!--Contributors -->
			<section id="Chapter 9.3"data-background-color="white">
				<h2> 9.3 Contributors</h2>
				<div style="text-align: left; font-size: 75%">
					<ul>
						<li style="margin-bottom: 2%;"> <b>Course material authors:</b> Florian Albrecht, Spatial Service GmbH, Salzburg, Austria, Vanessa Streifeneder and Stefan Lang, University of Salzburg, Department of Geoinformatics - Z_GIS, Salzburg, Austria</li>
						<li style="margin-bottom: 2%;"> <b>TensorFlow Script authors:</b> Vanessa Streifeneder, Getachew Workineh Gella, Yunya Gao, University of Salzburg, Department of Geoinformatics - Z_GIS, Salzburg, Austria</li>
						<li style="margin-bottom: 2%;"> <b>PyTorch Script author:</b> Getachew Workineh Gella, University of Salzburg, Department of Geoinformatics - Z_GIS, Salzburg, Austria</li>
						<li> <b>Speaker:</b> Hannah Augustin, University of Salzburg, Department of Geoinformatics - Z_GIS, Salzburg, Austria</li>
					</ul>
				</div>
				
			</section>

		</section>
			
		
			

</div>


		

		
	<script>Reveal.initialize({
				simplemenu: {
					menuselector: '.menu a'
				},
				dependencies: [
					{ src: './plugin/markdown/assets/js/revealjs/plugin/simplemenu/simplemenu.js', async: false }
				]
			});
		</script>

		<div class="progress" style="display: block;"><span style="width: 78.8387px;"></span></div>
		<aside class="controls" data-controls-layout="bottom-right" data-controls-back-arrows="faded"
			style="display: block;">
			<button class="navigate-left" aria-label="previous slide" disabled="disabled">
				<div class="controls-arrow"></div>
			</button>
			<button class="navigate-right enabled" aria-label="next slide">
				<div class="controls-arrow"></div>
			</button><button class="navigate-up" aria-label="above slide" disabled="disabled">
				<div class="controls-arrow"></div>
			</button><button class="navigate-down" aria-label="below slide" disabled="disabled">
				<div class="controls-arrow"></div>
			</button>
		</aside >	
	
		
	
		<!--<div class="slide-number" style="display: block;">
		</div>-->
		<div class="speaker-notes" data-prevent-swipe="" tabindex="0"></div>
		<div class="pause-overlay"><button class="resume-button">Resume presentation</button></div>
		<div id="aria-status-div" aria-live="polite" aria-atomic="true"
			style="position: absolute; height: 1px; width: 1px; overflow: hidden; clip: rect(1px, 1px, 1px, 1px);">
			Object-based Image Analysis (OBIA)
			An introductory course
			Stefan LANG & Dirk TIEDE

			[1] Why spatial image analysis? | [2] Regions and image objects | [3] Image segmentation | [4] Knowledge
			representation | [5] Class modelling
			University of Salzburg, Department of Geoinformatics, 2010-2022


		</div>			
		<div class="backgrounds">
			<div class="slide-background present" data-loaded="true" style="display: block;">
				<div class="slide-background-content"></div>
			</div>
			<div class="slide-background stack future" data-loaded="true" style="display: block;">
				<div class="slide-background-content"></div>
				<div class="slide-background present" data-loaded="true" style="display: block;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" data-loaded="true" style="display: block;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
			</div>
			<div class="slide-background stack future" data-loaded="true" style="display: block;">
				<div class="slide-background-content"></div>
				<div class="slide-background present" data-loaded="true" style="display: block;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
			</div>
			<div class="slide-background stack future" style="display: none;" data-loaded="true">
				<div class="slide-background-content"></div>
				<div class="slide-background present" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
			</div>
			<div class="slide-background stack future" style="display: none;" data-loaded="true">
				<div class="slide-background-content"></div>
				<div class="slide-background present" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
			</div>
			<div class="slide-background stack future" style="display: none;" data-loaded="true">
				<div class="slide-background-content"></div>
				<div class="slide-background present" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;" data-loaded="true">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
				<div class="slide-background future" style="display: none;">
					<div class="slide-background-content"></div>
				</div>
			</div>
			<div class="slide-background future" style="display: none;" data-loaded="true">
				<div class="slide-background-content"></div>
			</div>
		</div>
		
		
		<div class="slide-menu-botton">
			<i class="fas fa-bras"> </i>
	
		</div>
	
			
		
	</div>
	<div class="footer" style="position: absolute; bottom: 0%; margin-top: 2%; ">
			
		<audio controls id='audio'  >
			<source src='./audio/'  type="audio/mp3"> </source>
		</audio>
		
	</div>
	<script src="./js/reveal.js"></script>
	
	<script src="./plugin/pdfexport/pdfexport.js"></script>
	




		<script>
			
			Reveal.initialize({
				//width: 100% ,
			//	height:100% ,
				//margin: 0.04,
				//minScale: 0.2,
				//maxScale: 2.0,
				controls: true,
				progress: true,
				history: true,
				center: true,
				embedded: true,
				zoom:true,
				//autoSlide:20000,
				//loop:true,
				//transition:'slide',
				slideNumber:'h/v',
				pdfExportShortcut: 'E',
				
				
				
			
				

				// Optional libraries used to extend on reveal.js
				dependencies: [
					//{ src: './plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
					//{ src: './plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
					//{ src: './plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
					//{ src: './plugin/notes/notes.js' }, { src: 'socket.io/socket.io.js', async: true },
					//{ src: './plugin/notes-server/client.js', async: true },
					
					
					
				],
				plugins: [ RevealMenu, PdfExport],
				
				menu:{
					//numbers:'c',
					titleSelector: 'h1, h2',
					markers: true,
					//openSlideNumbers:false,
					themes: false,
					transition:false,
									}

				
			});
		
			
			
		Reveal.configure({
			pdfMaxPagesPerSlide:1,
			pdfSeparateFragments:true
		});

// audio-player
		Reveal.on('slidechanged', function (event){
			
					var t=document.getElementById("audio");
					console.log(t)
					
					var h=event.indexh;
					var v=event.indexv;
					
					var a=+h+"."+v+".mp3";
					tSrc=encodeURIComponent(a);
					
					t.src="./audio/"+tSrc;
					
				
					
					});

// audio-player: first slide	
		Reveal.on('ready', function (event){
			
			var t=document.getElementById("audio");
			console.log(t)
			
			var h=event.indexh;
			var v=event.indexv;
			if(h==0 && v==0){

			
			var a=+h+"."+v+".mp3";
			tSrc=encodeURIComponent(a);
			
			t.src="./audio/"+tSrc;
		}
		
			
			});
		
			
				
		
	
				
        

			
		</script>
		
	
</body>

</html>
